{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importando as bibliotecas utilizadas ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ANACONDA\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import gensim\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializando o DataFrame da matriz dos conhecimentos, \n",
    "utilizando a função 'fillna' para consertar as células mescladas,\n",
    "lendo a partir da linha 4\n",
    "e lendo até a coluna 9 (excluso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "crea_df = pd.read_excel(\"Matriz_do_Conhecimento.xls\", skiprows=4).iloc[:, :9].fillna(method=\"ffill\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corrigindo valores que foram incorretamente preenchido pelo método fillna(\"ffill\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "crea_df = crea_df.replace({\"TÓPICOS\": np.NaN, \"Nº DE ORDEM DOS TÓPICOS\": np.NaN})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando o Dataframe com o pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('crea_df_pickle.txt', 'wb') as f:\n",
    "    pickle.dump(crea_df, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abrindo o dataFrame em formato Binário e Imprimindo o DataFrame para fins de visualização:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('crea_df_pickle.txt', 'rb') as f:\n",
    "    crea_df = pickle.load(f)\n",
    "\n",
    "# * subseção de Construção de Edificações\n",
    "subsection1_df = crea_df[ crea_df['SUB-SETOR' ] == 'Construção de Edificações']\n",
    "\n",
    "mechatronics_df = crea_df[ crea_df['SETOR'] == 'Controle e Automação']\n",
    "\n",
    "# mechatronics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importando as matérias que foram manualmente obtidas e salvas em um arquivo json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_df = pd.read_json(\"MechatronicsEngeneeringSubjects.json\")\n",
    "\n",
    "# subjects_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando uma função de pre-processamento de texto. Retira ruidos(cleaning) -> tokeniza-lemmatiza -> depois retira stopwords;\n",
    "\n",
    "Recebe um texto no formato de string e retorna uma lista de strings com as palavras do documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "nlp = spacy.load('pt_core_news_lg')\n",
    "\n",
    "# TODO: find out if it is definately impossible to disable PoS tags in nlp model; alternative: convert all verbs to noun using wordnet and .pos from spacy?\n",
    "# TODO: implementar o tagger utilizando o seguinte banco de dados: https://www.nltk.org/howto/portuguese_en.html#accessing-the-macmorpho-tagged-corpus\n",
    "# tagger = nlp.get_pipe(\"tagger\")\n",
    "# doc = nlp(\"eletromagnetismo serie\")\n",
    "# print(tagger.model.predict([doc])[0][1])\n",
    "# print(tagger.labels)\n",
    "\n",
    "# * adding custom texts that dont represent real words\n",
    "noises_list = [\"i\", \"ii\", \"iii\", \"iv\", \"v\", \"vi\", \"vii\", \"viii\", \"ix\", \"x\", \"xi\"]\n",
    "\n",
    "stopWords_list = stopwords.words(\"portuguese\")\n",
    "\n",
    "# * adding custom words to StopWords list\n",
    "stopWords_list += [\n",
    "    'referente',\n",
    "    'seguinte',\n",
    "    'etc',\n",
    "    'ª',\n",
    "    'tal',\n",
    "    'um', \n",
    "    'dois',\n",
    "    'tres',\n",
    "    'vs',\n",
    "    'aula',\n",
    "    'tal',\n",
    "]\n",
    "\n",
    "# * preprocessing stopwords to correct format\n",
    "stopWords_list = gensim.utils.simple_preprocess(\" \".join(stopWords_list), deacc=True, min_len=1, max_len=40)\n",
    "\n",
    "# print(stopWords_list)\n",
    "\n",
    "# * manual intervention, changing final lemmas\n",
    "intervention_dict = {\n",
    "    \"campar\": \"campo\",\n",
    "    \"seriar\":\"serie\",\n",
    "    \"eletromagnetico\":\"eletromagnetismo\",\n",
    "}\n",
    "\n",
    "def preprocess(text):\n",
    "    # * importing stopwords from nltk and spacy pipeline\n",
    "    global nlp\n",
    "    global stopWords_list\n",
    "    global noises_list\n",
    "    global intervention_dict\n",
    "\n",
    "    # * preprocessing text with gensim.simple_preprocess, eliminating noises: lowercase, tokenized, no symbols, no numbers, no accents marks(normatize)\n",
    "    text_list = gensim.utils.simple_preprocess(text, deacc=True, min_len=1, max_len=40)\n",
    "\n",
    "    # * recombining tokens to a string type object and removing remaining noises\n",
    "    text_str = \" \".join([word for word in text_list if word not in noises_list])\n",
    "\n",
    "    # * preprocessing with spacy, retokenizing -> tagging parts of speech (PoS) -> parsing (assigning dependencies between words) -> lemmatizing\n",
    "    text_doc = nlp(text_str)\n",
    "\n",
    "    # * re-tokenization, removing stopwords and lemmatizing\n",
    "    lemmatized_text_list = [token.lemma_ for token in text_doc if token.text not in stopWords_list]\n",
    "\n",
    "    # * manual intervention conversion of lemmas and removing 1 letter stopwords\n",
    "    output = []\n",
    "    for token in lemmatized_text_list:\n",
    "        if len(token) <= 1:\n",
    "            continue\n",
    "        if token in intervention_dict:\n",
    "            output.append(intervention_dict[token])\n",
    "        else:\n",
    "            output.append(token)\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processando as matérias com Stopwords do NLTK e função do Gensim. E adicionando uma nova columa ao Dataframe que consiste em todo o texto da matéria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_list = []\n",
    "\n",
    "for i, row in subjects_df.iterrows():\n",
    "\n",
    "    # * reading values of each subject (row)\n",
    "    subject_id = row[\"codigo\"]\n",
    "    name = row[\"nome\"]\n",
    "    syllabus = row[\"ementa\"]\n",
    "    content = row[\"conteudo\"]\n",
    "\n",
    "    # * combining them to create the subject document\n",
    "    text = name + ' ' + syllabus + ' ' + content\n",
    "    \n",
    "    # * preprocessing\n",
    "    preProcessedText = preprocess(text)\n",
    "    documents_list.append(preProcessedText)\n",
    "\n",
    "# print(documents_list)\n",
    "\n",
    "documents_series = pd.Series(documents_list, name=\"documento\")\n",
    "\n",
    "documents_df = pd.concat([subjects_df, documents_series], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando o Dataframe das matérias,  reabrindo-o e imprimindo o DataFrame para fins de visualização:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('documents_df.txt', 'wb') as f:\n",
    "    pickle.dump(documents_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('documents_df.txt', 'rb') as f:\n",
    "    documents_df = pickle.load(f)\n",
    "\n",
    "# documents_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando a coluna de documentos para tornar legível e facilitar a busca, assim podendo conferir se os lemmas estão satisfatórios e se condizem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"debbuging_docs.json\", \"w+\") as f:\n",
    "    json.dump(documents_df['documento'].to_dict(), f, indent=4, ensure_ascii=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "construindo o Corpus, vetorizando os documentos com o bag-of-words do gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "lemmatizedData = documents_df[\"documento\"].tolist()\n",
    "\n",
    "# * gensim dictionary object, which will track each word to its respective id\n",
    "id2wordDict = corpora.Dictionary(lemmatizedData)\n",
    "\n",
    "# * gensim doc2bow method to map each word to a integer id and its respective frequency\n",
    "corpus = [id2wordDict.doc2bow(text) for text in lemmatizedData]\n",
    "\n",
    "# * list of list of tuples (id of a word, frequency)\n",
    "# corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando tf-idf do gensim para descobrir as palavras mais importantes e dar mais pesos a elas;\n",
    "\n",
    "tupla(int, int) -> tupla(int, float)\n",
    "\n",
    "td-idf(term_i, document_j) = freq(i,j) * log2 ( inverse_document_frequency(i) )\n",
    "\n",
    "freq(i, j) = total occurances of i in j / total words in j\n",
    "\n",
    "inverse_document_frequency(i) = total documents / documents that have at least one occurance of i \n",
    "\n",
    "O parâmetro id2word recebe o dicionário que mapeia as palavras com os respectivos IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model = gensim.models.TfidfModel(corpus, id2word=id2wordDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSI, pega a matriz do corpus e a decompõe utilizando o SVD. Das três matrizes criadas, utiliza-se apenas o right singular vectors que representa a relação entre os tópicos latentes com as palavras.\n",
    "\n",
    "O parâmetro id2word recebe o dicionário que mapeia as palavras com os respectivos IDs. E o parâmetro power_iters define o número de iterações para treinamento do modelo, e, portanto, quanto maior o valor, mais acurado e mais devagar vai ser o treino modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_model = gensim.models.LsiModel(tfidf_model[corpus], id2word=id2wordDict, num_topics=100, power_iters=100)\n",
    "\n",
    "# lsi_model.print_topics(num_topics=56)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "left singular vector -> term-to-topic matrix (não será utilizado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2030, 57)\n"
     ]
    }
   ],
   "source": [
    "# * U Matrix\n",
    "print(np.shape(lsi_model.projection.u))\n",
    "\n",
    "# lsi_model.projection.u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "singular values -> \"impacto\" (abrangência?) de cada tópico -> \"feature importance\"\n",
    "\n",
    "Pode-se escolher um valor de corte para reduzir o valor do parametro num_topics, aumentando, assim, o desempenho de tempo. O número de tópicos latentes não tem como ser maior que o numero de documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "número de documentos/ementas/matérias: 58\n",
      "topicos latentes encontrados com um 'impacto alto': 57\n"
     ]
    }
   ],
   "source": [
    "print(\"número de documentos/ementas/matérias:\", len(subjects_df))\n",
    "print(\"topicos latentes encontrados com um \\'impacto alto\\':\", len(lsi_model.projection.s))\n",
    "\n",
    "# * S matrix (sigma)\n",
    "# print(np.shape(lsi_model.projection.s))\n",
    "# lsi_model.projection.s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "right singular vectors -> document-to-topic matrix (Não é diretamente calculado e armazenado pois pode ser muito grande devido a quantidade de documentos -> num_topic x documents)\n",
    "\n",
    "Será o documento armazenado e utilizado para fazer as queries de similaridade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58, 57)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-8.16842379e-02, -1.90699447e-02,  3.26055870e-03, ...,\n",
       "        -1.17165794e-02,  1.53363475e-02, -4.56397882e-03],\n",
       "       [-1.87783694e-01,  1.58409031e-02, -3.87895189e-01, ...,\n",
       "        -2.08063679e-02, -1.61471958e-02, -2.67343672e-02],\n",
       "       [-1.26493891e-01,  1.25987436e-02, -2.75186795e-01, ...,\n",
       "         1.61533114e-02, -5.41333898e-03,  7.17352750e-03],\n",
       "       ...,\n",
       "       [-6.52923932e-02, -3.26972188e-02, -3.83349938e-02, ...,\n",
       "         9.92697377e-04,  3.13467224e-04, -5.76186915e-04],\n",
       "       [-2.21970532e-02,  2.30684218e-03, -1.23568911e-02, ...,\n",
       "         4.02028368e-03, -5.54040801e-04,  7.26040521e-04],\n",
       "       [-1.34881176e-02, -1.68342006e-02, -2.67376240e-03, ...,\n",
       "         1.11045832e-03, -1.30478494e-03,  3.83274140e-04]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_matrix = gensim.matutils.corpus2dense(lsi_model[tfidf_model[corpus]], len(lsi_model.projection.s)).T / lsi_model.projection.s\n",
    "print(np.shape(V_matrix))\n",
    "# * V or V^T matrix\n",
    "# * representado por lsi_model[x]\n",
    "V_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando, então, o corpus já processado pelo tf_idf e também a matrix V de LSI em um arquivo texto do tipo Matrix Market format, que permite que a matriz seja guardado em um arquivo texto, mas também pode ser utilizado para cálculos. (o método Serialize guardas os indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim.corpora.MmCorpus.serialize('tfidf_model_mm', tfidf_model[corpus])\n",
    "gensim.corpora.MmCorpus.serialize('lsi_model_mm', lsi_model[tfidf_model[corpus]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abrindo as matrizes salvas nos arquivos textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MmCorpus(58 documents, 2030 features, 4329 non-zero entries)\n",
      "MmCorpus(58 documents, 57 features, 3306 non-zero entries)\n"
     ]
    }
   ],
   "source": [
    "tfidf_corpus = gensim.corpora.MmCorpus('tfidf_model_mm')\n",
    "lsi_corpus = gensim.corpora.MmCorpus('lsi_model_mm')\n",
    "\n",
    "# * features of td-idf matrix are the different words\n",
    "print(tfidf_corpus)\n",
    "\n",
    "# * features of LSI model are the latent topics discovered\n",
    "print(lsi_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buscas de Similaridade Semântica utilizando o método Matrix similarity do gensim, o qual computa o \"cosine similarity\".\n",
    "\n",
    "Se forem buscados termos que não foram adicionados ao dicionário id2wordDict, não há como mapear esse documento\n",
    "\n",
    "O parâmetro num_best define quantos dos documentos mais proximos será buscado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.similarities.docsim import MatrixSimilarity\n",
    "\n",
    "cosineSimilarity = MatrixSimilarity(lsi_corpus, num_features = lsi_corpus.num_terms, num_best=8)\n",
    "\n",
    "def search_similarity_query(search_document):\n",
    "\n",
    "    # * preprocessing and processing until becomes a matrix of type term_to_topic (V)\n",
    "    doc = preprocess(search_document)\n",
    "    query_bow = id2wordDict.doc2bow(doc)\n",
    "    query_tfidf = tfidf_model[query_bow]\n",
    "    query_lsi = lsi_model[query_tfidf]\n",
    "\n",
    "    # * cossine similarity between the vector of the new document vs all other vectors of documents\n",
    "    # * returns a list of tuples (id of compared document, similarity)\n",
    "    ranking = cosineSimilarity[query_lsi]\n",
    "\n",
    "    ranking.sort(key=lambda unit: unit[1], reverse= True)\n",
    "    result = []\n",
    "\n",
    "    for subject in ranking:\n",
    "\n",
    "        result.append (\n",
    "            {\n",
    "                'Relevancia': round((subject[1] * 100),6),\n",
    "                'Código da Matéria': subjects_df['codigo'][subject[0]],\n",
    "                'Nome da matéria': subjects_df['nome'][subject[0]]\n",
    "            }\n",
    "\n",
    "        )\n",
    "    \n",
    "    output = pd.DataFrame(result, columns=['Relevancia','Código da Matéria','Nome da matéria'])\n",
    "    \n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando documentos no formato de str, pode-se realizar buscas de similaridade semântica entre as matérias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eletromagnetismo']\n"
     ]
    }
   ],
   "source": [
    "search_similarity_query(\"eletromagnético\")\n",
    "print(preprocess(\"eletromagnético\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2211, 9)\n"
     ]
    }
   ],
   "source": [
    "print( mechatronics_df.shape )\n",
    "\n",
    "query_full_text = \"\"\n",
    "visited_topics = {}\n",
    "for i in range(2211):\n",
    "    text = mechatronics_df.iloc[i, 8] + ' ' + mechatronics_df.iloc[i, 7]\n",
    "    if text in visited_topics:\n",
    "        continue\n",
    "    visited_topics[text] = (mechatronics_df.iloc[i, 8],  mechatronics_df.iloc[i, 7])\n",
    "    # print(f'conteúdo buscado: {mechatronics_df.iloc[i, 8]},\\nárea do conhecimento: {mechatronics_df.iloc[i, 7]}')\n",
    "    # print(f'texto buscado depois de preprocessado: {preprocess(text)}')\n",
    "    # print( search_similarity_query(text), end='\\n\\n')\n",
    "\n",
    "    query_full_text += \"Index: \" + str(i) + '\\n'\n",
    "    query_full_text += \"Conteúdo buscado: \" + mechatronics_df.iloc[i, 8] + '\\n'\n",
    "    query_full_text += \"área do conhecimento: \" + mechatronics_df.iloc[i, 7] + '\\n'\n",
    "    query_full_text += \"Texto buscado depois de preprocessado: \" + str(preprocess(text)) + '\\n'\n",
    "    query_full_text += search_similarity_query(text).to_string() + \"\\n\\n\"\n",
    "\n",
    "with open(\"mechatronics_requirements_query.txt\", 'w') as f:\n",
    "    f.write(query_full_text)\n",
    "\n",
    "# print(len(visited_topics))\n",
    "# for i in visited_topics.items():\n",
    "#     print(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing all topics and syllabus into a xlsx file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlsxwriter\n",
    "\n",
    "workbook = xlsxwriter.Workbook('conteudos_e_ementas.xlsx')\n",
    "\n",
    "worksheet1 = workbook.add_worksheet()\n",
    "\n",
    "worksheet1.write(1, 1, \"Código da Matéria\")\n",
    "worksheet1.write(1, 2, \"Nome da Matéria\")\n",
    "row = 2\n",
    "    \n",
    "for i, line in subjects_df.iterrows():\n",
    "    worksheet1.write(row, 1, line[\"codigo\"])\n",
    "    worksheet1.write(row, 2, line[\"nome\"])\n",
    "    row += 1\n",
    "\n",
    "worksheet1.write(1, 4, \"Conteúdos\")\n",
    "worksheet1.write(1, 5, \"Área do Conhecimento\")\n",
    "worksheet1.write(1, 6, \"Ementa(s) Associadas\")\n",
    "row = 2\n",
    "for item in visited_topics.values():\n",
    "    worksheet1.write(row, 4, item[0])\n",
    "    worksheet1.write(row, 5, item[1])\n",
    "    row += 1\n",
    "\n",
    "workbook.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ae7890921ac3c17143ff000ac7152addc6614e2051824da39aa37dab63d26d82"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
