{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importando as matérias do curso de Mecatrônica, processando-os e adicionando uma coluna de \"Documentos\" no Dataframe das Matérias:\n",
    "\n",
    "Esse dataframe será salvo no arquivo: \"documents_df.txt\" no formato binário do Pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ANACONDA\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import gensim\n",
    "import spacy\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nlp = spacy.load('pt_core_news_lg')\n",
    "\n",
    "# * adding custom texts that dont represent real words\n",
    "noises_list = [\"i\", \"ii\", \"iii\", \"iv\", \"v\", \"vi\", \"vii\", \"viii\", \"ix\", \"x\", \"xi\"]\n",
    "\n",
    "stopWords_list = stopwords.words(\"portuguese\")\n",
    "\n",
    "# * adding custom words to StopWords list\n",
    "stopWords_list += [\n",
    "    'referente',\n",
    "    'seguinte'\n",
    "]\n",
    "\n",
    "# * preprocessing stopwords to correct format\n",
    "stopWords_list = gensim.utils.simple_preprocess(\" \".join(stopWords_list), deacc=True, min_len=1, max_len=40)\n",
    "\n",
    "# * manual intervention, changing final lemmas\n",
    "intervention_dict = {\n",
    "    \"campar\": \"campo\",\n",
    "    \"seriar\":\"serie\",\n",
    "    \"eletromagnetico\":\"eletromagnetismo\",\n",
    "}\n",
    "\n",
    "def preprocess(text):\n",
    "    # * importing stopwords from nltk and spacy pipeline\n",
    "    global nlp\n",
    "    global stopWords_list\n",
    "    global noises_list\n",
    "    global intervention_dict\n",
    "\n",
    "    # * preprocessing text with gensim.simple_preprocess, eliminating noises: lowercase, tokenized, no symbols, no numbers, no accents marks(normatize)\n",
    "    text_list = gensim.utils.simple_preprocess(text, deacc=True, min_len=1, max_len=40)\n",
    "\n",
    "    # * recombining tokens to a string type object and removing remaining noises\n",
    "    text_str = \" \".join([word for word in text_list if word not in noises_list])\n",
    "\n",
    "    # * preprocessing with spacy, retokenizing -> tagging parts of speech (PoS) -> parsing (assigning dependencies between words) -> lemmatizing\n",
    "    text_doc = nlp(text_str)\n",
    "\n",
    "    # * re-tokenization, removing stopwords and lemmatizing\n",
    "    lemmatized_text_list = [token.lemma_ for token in text_doc if token.text not in stopWords_list]\n",
    "\n",
    "    # * manual intervention conversion of lemmas\n",
    "    output = []\n",
    "    for token in lemmatized_text_list:\n",
    "        if token in intervention_dict:\n",
    "            output.append(intervention_dict[token])\n",
    "        else:\n",
    "            output.append(token)\n",
    "            \n",
    "    return output\n",
    "\n",
    "subjects_df = pd.read_json(\"MechatronicsEngeneeringSubjects.json\")\n",
    "\n",
    "documents_list = []\n",
    "\n",
    "for i, row in subjects_df.iterrows():\n",
    "\n",
    "    # * reading values of each subject (row)\n",
    "    subject_id = row[\"codigo\"]\n",
    "    name = row[\"nome\"]\n",
    "    syllabus = row[\"ementa\"]\n",
    "    content = row[\"conteudo\"]\n",
    "\n",
    "    # * combining them to create the subject document\n",
    "    text = name + ' ' + syllabus + ' ' + content\n",
    "    \n",
    "    # * preprocessing\n",
    "    preProcessedText = preprocess(text)\n",
    "    documents_list.append(preProcessedText)\n",
    "\n",
    "documents_series = pd.Series(documents_list, name=\"documento\")\n",
    "\n",
    "documents_df = pd.concat([subjects_df, documents_series], axis=1)\n",
    "\n",
    "with open('documents_df.txt', 'wb') as f:\n",
    "    pickle.dump(documents_df, f)\n",
    "\n",
    "# * crea topics\n",
    "crea_df = pd.read_excel(\"Matriz_do_Conhecimento.xls\", skiprows=4).iloc[:, :9].fillna(method=\"ffill\")\n",
    "crea_df = crea_df.replace({\"TÓPICOS\": np.NaN, \"Nº DE ORDEM DOS TÓPICOS\": np.NaN})\n",
    "\n",
    "with open('crea_df_pickle.txt', 'wb') as f:\n",
    "    pickle.dump(crea_df, f)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ae7890921ac3c17143ff000ac7152addc6614e2051824da39aa37dab63d26d82"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
